{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM-based Anomaly Detection for Time Series Data\n",
    "\n",
    "This notebook demonstrates the end-to-end process of building an optimized anomaly detection system using an LSTM Autoencoder.\n",
    "\n",
    "## 1. Setup and Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated data/cmapss/train_FD001.txt\n",
      "Generated data/cmapss/test_FD001.txt\n",
      "Data generation complete.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def generate_synthetic_cmapss(filename, num_units=10, max_cycles=200):\n",
    "    np.random.seed(42)\n",
    "    data = []\n",
    "    \n",
    "    for unit_id in range(1, num_units + 1):\n",
    "        # Each unit has a random lifespan between 150 and max_cycles\n",
    "        lifespan = np.random.randint(150, max_cycles + 1)\n",
    "        \n",
    "        for cycle in range(1, lifespan + 1):\n",
    " \n",
    "            s1 = 518.67  # constant\n",
    " \n",
    "            s2 = 641.81 - 5 * (cycle / lifespan) + np.random.normal(0, 0.1)\n",
    "\n",
    "            s3 = 1589.70 + np.random.normal(0, 0.5) # noise\n",
    " \n",
    "            trend = (cycle / lifespan) ** 2\n",
    " \n",
    "            s4 = 1400.60 + 40 * trend + np.random.normal(0, 0.2)\n",
    "   \n",
    "            s5 = 14.62\n",
    " \n",
    "            s6 = 21.61 + 2 * trend + np.random.normal(0, 0.01)\n",
    " \n",
    "            set1 = np.random.normal(0, 0.001)\n",
    "            set2 = np.random.normal(0, 0.001)\n",
    "            set3 = 100.0\n",
    "            \n",
    "            row = [unit_id, cycle, set1, set2, set3, s1, s2, s3, s4, s5, s6]\n",
    "            data.append(row)\n",
    "            \n",
    "    columns = ['unit_id', 'cycle', 'setting1', 'setting2', 'setting3', \n",
    "               's1', 's2', 's3', 's4', 's5', 's6']\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "    df.to_csv(filename, sep=' ', index=False, header=False)\n",
    "    print(f\"Generated {filename}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    os.makedirs('data/cmapss', exist_ok=True)\n",
    "    generate_synthetic_cmapss('data/cmapss/train_FD001.txt', num_units=100)\n",
    "    generate_synthetic_cmapss('data/cmapss/test_FD001.txt', num_units=20)\n",
    "\n",
    "print(\"Data generation complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing\n",
    "We split the data into sequences. Crucially, for training, we only use the first 80% of each unit's life to ensure the model learns 'healthy' patterns only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (13061, 10, 9)\n",
      "X_test shape: (3396, 10, 9)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "def load_data(file_path):\n",
    "    columns = ['unit_id', 'cycle', 'setting1', 'setting2', 'setting3', \n",
    "               's1', 's2', 's3', 's4', 's5', 's6']\n",
    "    df = pd.read_csv(file_path, sep=' ', header=None, names=columns)\n",
    "    return df\n",
    "\n",
    "def preprocess_data(train_df, test_df):\n",
    "    # Select features (excluding unit_id and cycle for training input)\n",
    "    features = ['setting1', 'setting2', 'setting3', 's1', 's2', 's3', 's4', 's5', 's6']\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    train_df[features] = scaler.fit_transform(train_df[features])\n",
    "    test_df[features] = scaler.transform(test_df[features])\n",
    "    \n",
    "    # Save scaler for later use in dashboard\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "    joblib.dump(scaler, 'models/scaler.joblib')\n",
    "    \n",
    "    return train_df, test_df, features\n",
    "\n",
    "def create_sequences(data, seq_length, training_mode=False):\n",
    "    sequences = []\n",
    "    for unit_id in data['unit_id'].unique():\n",
    "        unit_data_df = data[data['unit_id'] == unit_id]\n",
    "        \n",
    "        # If training, only use the first 80% of data (assumed healthy)\n",
    "        if training_mode:\n",
    "            cutoff = int(len(unit_data_df) * 0.80)\n",
    "            unit_data = unit_data_df.iloc[:cutoff, 2:].values\n",
    "        else:\n",
    "            unit_data = unit_data_df.iloc[:, 2:].values\n",
    "            \n",
    "        if len(unit_data) >= seq_length:\n",
    "            for i in range(len(unit_data) - seq_length + 1):\n",
    "                sequences.append(unit_data[i:i+seq_length])\n",
    "    return np.array(sequences)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_df = load_data('data/cmapss/train_FD001.txt')\n",
    "    test_df = load_data('data/cmapss/test_FD001.txt')\n",
    "    \n",
    "    train_df, test_df, features = preprocess_data(train_df, test_df)\n",
    "    \n",
    "    SEQ_LENGTH = 10\n",
    "    # Enable training_mode=True for X_train to learn only from healthy data\n",
    "    X_train = create_sequences(train_df, SEQ_LENGTH, training_mode=True)\n",
    "    X_test = create_sequences(test_df, SEQ_LENGTH, training_mode=False)\n",
    "    \n",
    "    print(f\"X_train shape: {X_train.shape}\")\n",
    "    print(f\"X_test shape: {X_test.shape}\")\n",
    "    \n",
    "    np.save('data/X_train.npy', X_train)\n",
    "    np.save('data/X_test.npy', X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Training\n",
    "We train an LSTM Autoencoder with Dropout layers to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LOKESH\\anaconda3\\envs\\helloTF-tf\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)              │          <span style=\"color: #00af00; text-decoration-color: #00af00\">18,944</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">12,416</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ repeat_vector (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">RepeatVector</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)              │          <span style=\"color: #00af00; text-decoration-color: #00af00\">24,832</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ time_distributed (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>)               │             <span style=\"color: #00af00; text-decoration-color: #00af00\">585</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m64\u001b[0m)              │          \u001b[38;5;34m18,944\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m64\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │          \u001b[38;5;34m12,416\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ repeat_vector (\u001b[38;5;33mRepeatVector\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m32\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m32\u001b[0m)              │           \u001b[38;5;34m8,320\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m32\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m64\u001b[0m)              │          \u001b[38;5;34m24,832\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m64\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ time_distributed (\u001b[38;5;33mTimeDistributed\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m9\u001b[0m)               │             \u001b[38;5;34m585\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">65,097</span> (254.29 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m65,097\u001b[0m (254.29 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">65,097</span> (254.29 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m65,097\u001b[0m (254.29 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, RepeatVector, TimeDistributed, Dropout\n",
    "\n",
    "def build_lstm_autoencoder(seq_length, num_features):\n",
    "    model = Sequential([\n",
    "        # Encoder\n",
    "        LSTM(64, activation='relu', input_shape=(seq_length, num_features), return_sequences=True),\n",
    "        Dropout(0.2),\n",
    "        LSTM(32, activation='relu', return_sequences=False),\n",
    "        Dropout(0.2),\n",
    "        RepeatVector(seq_length),\n",
    "        # Decoder\n",
    "        LSTM(32, activation='relu', return_sequences=True),\n",
    "        Dropout(0.2),\n",
    "        LSTM(64, activation='relu', return_sequences=True),\n",
    "        Dropout(0.2),\n",
    "        TimeDistributed(Dense(num_features))\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mae')\n",
    "    return model\n",
    "\n",
    "model = build_lstm_autoencoder(10, 9)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 17ms/step - loss: 0.1378 - val_loss: 0.0404\n",
      "Epoch 2/50\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - loss: 0.0483 - val_loss: 0.0386\n",
      "Epoch 3/50\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 15ms/step - loss: 0.0444 - val_loss: 0.0370\n",
      "Epoch 4/50\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 15ms/step - loss: 0.0428 - val_loss: 0.0382\n",
      "Epoch 5/50\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 15ms/step - loss: 0.0418 - val_loss: 0.0376\n",
      "Epoch 6/50\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 15ms/step - loss: 0.0411 - val_loss: 0.0388\n",
      "Epoch 7/50\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - loss: 0.0406 - val_loss: 0.0380\n",
      "Epoch 8/50\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - loss: 0.0402 - val_loss: 0.0384\n",
      "Model saved to models/lstm_autoencoder.keras\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    X_train = np.load('data/X_train.npy')\n",
    "    \n",
    "    seq_length = X_train.shape[1]\n",
    "    num_features = X_train.shape[2]\n",
    "    \n",
    "    model = build_lstm_autoencoder(seq_length, num_features)\n",
    "    \n",
    "    # Early stopping to prevent overfitting\n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, mode='min')\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train, X_train,\n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        validation_split=0.1,\n",
    "        callbacks=[early_stop]\n",
    "    )\n",
    "    \n",
    "    os.makedirs('models', exist_ok=True)\n",
    "    model.save('models/lstm_autoencoder.keras')\n",
    "    print(\"Model saved to models/lstm_autoencoder.keras\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Anomaly Detection\n",
    "We calculate the reconstruction error and determine a threshold based on the 99th percentile of the training error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m409/409\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step\n",
      "Calculated threshold: 0.05056601769378456\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "Number of anomalies detected in test set: 661 out of 3396\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "def calculate_anomaly_scores(model, data):\n",
    "    reconstructions = model.predict(data)\n",
    "    # Mean Absolute Error per sequence\n",
    "    mse = np.mean(np.abs(reconstructions - data), axis=(1, 2))\n",
    "    return mse\n",
    "\n",
    "def find_threshold(model, train_data, percentile=99):\n",
    "    mse = calculate_anomaly_scores(model, train_data)\n",
    "    threshold = np.percentile(mse, percentile)\n",
    "    return threshold\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    X_train = np.load('data/X_train.npy')\n",
    "    X_test = np.load('data/X_test.npy')\n",
    "    \n",
    "    model = tf.keras.models.load_model('models/lstm_autoencoder.keras')\n",
    "    \n",
    "    threshold = find_threshold(model, X_train)\n",
    "    print(f\"Calculated threshold: {threshold}\")\n",
    "    \n",
    "    test_mse = calculate_anomaly_scores(model, X_test)\n",
    "    anomalies = test_mse > threshold\n",
    "    print(f\"Number of anomalies detected in test set: {np.sum(anomalies)} out of {len(X_test)}\")\n",
    "    \n",
    "    np.save('models/threshold.npy', np.array([threshold]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation\n",
    "Evaluating Precision, Recall, and F1 Score on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step\n",
      "\n",
      "Precision Value: 0.8260\n",
      "Recall Value: 0.9964\n",
      "F1 Score Value: 0.9032\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import tensorflow as tf\n",
    "\n",
    "def evaluate():\n",
    "    X_test = np.load('data/X_test.npy')\n",
    "    threshold = np.load('models/threshold.npy')[0]\n",
    "    model = tf.keras.models.load_model('models/lstm_autoencoder.keras')\n",
    "    \n",
    "    reconstructions = model.predict(X_test)\n",
    "    mse = np.mean(np.abs(reconstructions - X_test), axis=(1, 2))\n",
    "    predictions = mse > threshold\n",
    "    \n",
    "    # Define ground truth: For synthetic data, let's say the last 20% of cycles for each unit are anomalies\n",
    "    # We need to map sequences back to their units to do this properly, \n",
    "    # but for a simple evaluation, we can estimate.\n",
    "    \n",
    "    # Load original test data to get unit info\n",
    "    columns = ['unit_id', 'cycle', 'setting1', 'setting2', 'setting3', \n",
    "               's1', 's2', 's3', 's4', 's5', 's6']\n",
    "    test_df = pd.read_csv('data/cmapss/test_FD001.txt', sep=' ', header=None, names=columns)\n",
    "    \n",
    "    seq_length = 10\n",
    "    gt_anomalies = []\n",
    "    \n",
    "    for unit_id in test_df['unit_id'].unique():\n",
    "        unit_data = test_df[test_df['unit_id'] == unit_id]\n",
    "        lifespan = len(unit_data)\n",
    "        # Mark last 15% as anomalous\n",
    "        anomaly_start = int(lifespan * 0.85)\n",
    "        \n",
    "        unit_gt = [1 if i >= anomaly_start else 0 for i in range(lifespan)]\n",
    "        # sequences start from 0 to lifespan-seq_length\n",
    "        if lifespan >= seq_length:\n",
    "            for i in range(lifespan - seq_length + 1):\n",
    "                # sequence is anomalous if its last point is in the anomaly zone\n",
    "                gt_anomalies.append(unit_gt[i + seq_length - 1])\n",
    "                \n",
    "    gt_anomalies = np.array(gt_anomalies)\n",
    "    \n",
    "    # Align lengths if necessary (should match if logic is correct)\n",
    "    min_len = min(len(predictions), len(gt_anomalies))\n",
    "    predictions = predictions[:min_len]\n",
    "    gt_anomalies = gt_anomalies[:min_len]\n",
    "    \n",
    "    precision = precision_score(gt_anomalies, predictions)\n",
    "    recall = recall_score(gt_anomalies, predictions)\n",
    "    f1 = f1_score(gt_anomalies, predictions)\n",
    "    \n",
    "    print(f\"\\nPrecision Value: {precision:.4f}\")\n",
    "    print(f\"Recall Value: {recall:.4f}\")\n",
    "    print(f\"F1 Score Value: {f1:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
